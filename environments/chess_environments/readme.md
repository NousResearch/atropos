# â™Ÿï¸ Chess LLM Training Environments

**Author:** EaswarGn

---

## ğŸ“ Overview

This repository contains three custom reinforcement learning environments designed to fine-tune Large Language Models (LLMs) to understand and solve chess puzzles.

The main environment trains models to solve chess puzzles with Stockfish-aligned rewards.

Two pre-training environments progressively teach the model to:

1. Play legal moves âš–ï¸
2. Visualize multiple moves ahead ğŸ”®

This ensures a strong foundation before puzzle-solving fine-tuning.

Think of it as a **curriculum for chess-trained LLMs**:

> Learn the rules âš–ï¸ â†’ Learn to visualize ahead ğŸ”® â†’ Solve puzzles like a grandmaster ğŸ†

---

## ğŸ—ï¸ Environment Structure

### Core Components

- **`chess_rules_env.py`** â€“ teaches the LLM to generate legal chess moves.
- **`chess_board_visualization_env.py`** â€“ helps the LLM visualize moves ahead (lookahead training).
- **`chess_puzzle_env.py`** â€“ the main environment that fine-tunes the model to solve chess puzzles and compare its solutions with Stockfish using a novel reward system.

---

## ğŸ§© Environments in Detail

### 1ï¸âƒ£ Chess Rules Environment (`chess_rules_env`)

**ğŸ“š Goal:** Teach the LLM to play legal chess moves.

- Ensures model outputs valid UCI moves.
- Ideal as a first step before tackling puzzles.
- Model takes in board position and then outputs all possible legal moves, gets more reward for the more moves that it gets correct
- Uses my custom hugging face dataset to generate tough positions: https://huggingface.co/datasets/codingmonster1234/chess_puzzles_dataset

---

### 2ï¸âƒ£ Chess Board Visualization Environment (`chess_board_visualization_env`)

**ğŸ”® Goal:** Train the LLM to visualize multiple moves ahead, improving its internal chess â€œlookaheadâ€ ability.

- Provides scenarios requiring multi-move reasoning.
- Builds intuition for complex board states.
- Model recieves a sequence of moves and then must internally visualize the final board state and output it correctly. The reward is proportional to how close the model is to actual final position.
- Uses the custom ChessInstruct dataset: https://huggingface.co/datasets/Thytu/ChessInstruct

---

### 3ï¸âƒ£ Chess Puzzle Environment (`chess_puzzle_env`)

**â™Ÿï¸ Goal:** Fine-tune the model to solve real chess puzzles.

- Compares the modelâ€™s move sequence to Stockfishâ€™s evaluation.
- Uses a novel reward strategy to align the LLMâ€™s reasoning with engine-level accuracy.
- Supports step-by-step reasoning and reward shaping for deeper insights.
- Uses custom puzzle dataset curated from lichess.org: https://huggingface.co/datasets/codingmonster1234/chess_puzzles_dataset

## ğŸ† Stockfish-Based Reward Function

This environment uses a **novel reward function** to evaluate a model's predicted chess moves relative to the ground truth moves. Unlike a simple correct/incorrect metric, the reward measures **closeness to Stockfish evaluation**, encouraging the model to generate high-quality moves that align with expert evaluations.

---

### **How It Works**

1. **Inputs**
   - `initial_fen`: The starting board position in FEN format.
   - `pred_moves`: A sequence of predicted moves in UCI format.
   - `correct_moves`: The reference moves for the puzzle in UCI format.
   - `stockfish_path`: Path to the Stockfish engine.

2. **Step-wise Evaluation**
   - Each predicted move is played on a copy of the board.
   - The board is evaluated using Stockfish at a fixed depth (e.g., depth=12).
   - The predicted evaluation is compared with the evaluation of the correct move sequence.

3. **Aggregation**
   - Step rewards are summed and normalized over the total number of moves.
   - A final non-linear transformation is applied to keep the reward in `[0,1]`.


---

## ğŸ“Š Training Mechanics

**Curriculum Learning:** Recommended order
chess_rules_env â†’ chess_board_visualization_env â†’ chess_puzzle_env




**Reward Shaping:**

- **Rules env:** Rewards legal move generation. âœ…
- **Visualization env:** Rewards correct multi-move predictions. ğŸ”­
- **Puzzle env:** Rewards closeness to Stockfish evaluations. ğŸ…

**Evaluation Metrics:**

- Move legality âœ…
- Lookahead accuracy ğŸ”­
- Puzzle solution quality vs. Stockfish ğŸ…

## ğŸ”¬ Requirements
Only the chess library, pip install chess and atropos dependencies. Also, must download stockfish from: https://stockfishchess.org/download/


## ğŸ”¬ Research Applications

These environments can be used to explore key research questions in chess AI and learning:

1. **Chess Reasoning and Lookahead**
   - Study how LLMs develop multi-step planning and board visualization skills.
   - Analyze the effect of progressive training (rules â†’ visualization â†’ puzzles) on move prediction and strategy.

2. **Puzzle-Solving and Engine Alignment**
   - Investigate how closely LLMs can match Stockfish evaluations when solving chess puzzles.
   - Explore reward shaping strategies to align model reasoning with engine-level accuracy.

3. **Explainable Chess AI**
   - Examine step-by-step reasoning generated by LLMs.
   - Compare model move explanations to human or engine strategies for interpretability in chess decision-making.

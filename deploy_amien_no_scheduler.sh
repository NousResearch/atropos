#!/bin/bash
set -e

echo "ðŸš€ Deploying AMIEN to Google Cloud Platform (without Cloud Scheduler)"
echo "Project ID: amien-research-pipeline"
echo "Region: us-central1"

# Set project
gcloud config set project amien-research-pipeline

# Enable required APIs (excluding scheduler for now)
echo "ðŸ“¡ Enabling required APIs..."
gcloud services enable run.googleapis.com
gcloud services enable cloudbuild.googleapis.com
gcloud services enable storage.googleapis.com
gcloud services enable secretmanager.googleapis.com
gcloud services enable compute.googleapis.com
gcloud services enable monitoring.googleapis.com

# Create secrets (you'll need to add the actual API keys)
echo "ðŸ” Creating secrets..."
echo "$GEMINI_API_KEY" | gcloud secrets create gemini-api-key --data-file=- || echo "Secret already exists, updating..."
echo "$GEMINI_API_KEY" | gcloud secrets versions add gemini-api-key --data-file=- || echo "Failed to update secret"
echo "$PERPLEXITY_API_KEY" | gcloud secrets create perplexity-api-key --data-file=- || echo "Secret already exists, updating..."
echo "$PERPLEXITY_API_KEY" | gcloud secrets versions add perplexity-api-key --data-file=- || echo "Failed to update secret"

# Build and deploy container images
echo "ðŸ—ï¸ Building container images..."
cd gcp_deployment

# Prepare deployment files
echo "ðŸ“ Preparing deployment files..."

# Copy necessary files if they don't exist
if [ ! -f "requirements.txt" ]; then
    cp ../requirements_production.txt requirements.txt
fi

if [ ! -f "production_demo.py" ]; then
    cp ../production_demo.py .
fi

# Copy research modules
cp -r ../ai_research . 2>/dev/null || echo "ai_research directory not found, skipping"
cp -r ../evolution . 2>/dev/null || echo "evolution directory not found, skipping"
cp -r ../funsearch . 2>/dev/null || echo "funsearch directory not found, skipping"
cp -r ../AI-Scientist . 2>/dev/null || echo "AI-Scientist directory not found, skipping"

# Use the existing Dockerfile.api
if [ -f "Dockerfile.api" ]; then
    cp Dockerfile.api Dockerfile
else
    echo "âŒ Dockerfile.api not found, creating basic Dockerfile"
    cat > Dockerfile << 'EOF'
FROM python:3.10-slim

WORKDIR /app

# Copy requirements
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8080

# Run the application
CMD ["python", "main_api.py"]
EOF
fi

# Build API service
gcloud builds submit --tag gcr.io/amien-research-pipeline/amien-api-service .

# Deploy Cloud Run services
echo "â˜ï¸ Deploying Cloud Run services..."
gcloud run deploy amien-api-service \
    --image gcr.io/amien-research-pipeline/amien-api-service \
    --region us-central1 \
    --allow-unauthenticated \
    --memory 2Gi \
    --cpu 2 \
    --max-instances 100 \
    --set-env-vars="GCP_PROJECT_ID=amien-research-pipeline,AMIEN_ENV=production"

cd ..

echo "âœ… AMIEN core deployment complete!"
echo "ðŸŒ API URL: $(gcloud run services describe amien-api-service --region us-central1 --format='value(status.url)')"
echo "ðŸ“Š Monitor at: https://console.cloud.google.com/monitoring"

echo ""
echo "âš ï¸  Note: Cloud Scheduler was skipped due to permission issues."
echo "   You can enable it manually in the GCP Console:"
echo "   1. Go to https://console.cloud.google.com/cloudscheduler"
echo "   2. Enable the API if prompted"
echo "   3. Create jobs manually or run the scheduler setup script"

echo ""
echo "ðŸ”§ Next steps:"
echo "   1. Update API keys in secrets: gcloud secrets versions add gemini-api-key --data-file=<your-key-file>"
echo "   2. Test the deployment: curl \$(gcloud run services describe amien-api-service --region us-central1 --format='value(status.url)')/health"
echo "   3. Set up Cloud Scheduler manually if needed"
